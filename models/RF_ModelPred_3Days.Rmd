---
title: "RF_ModelPred_3Days"
output: html_document
date: "2023-05-09"
---

## Cleaning environment

```{r}
rm(list=ls())
```

# Random Forest Classifier

## Loading packages

```{r}
#loading packages
library(ggplot2)
library(lubridate)
library(tidyverse)
library(gridExtra)
library(dplyr)
library(hrbrthemes)
library(magrittr)
library(randomForest)
library(pROC)
library(MASS)
library(rlist)
library(e1071)
library(caTools)
library(class)
```

## Loading the data

```{r}
ModelData = read.csv(file.choose(),header=T)
```

## Get rid of the non-wanted columns and make different subsets for different models

```{r}
# ----------------------------------------------------------------------------------------------------------------
## Data for the model predicting if the error will happen within three days of the sensor reading
ModelData_Happen3 <- subset(ModelData, select=-c(TurbineName,interval_time, 
                                        Happen_1_day,Happen_2_days,Happen_one_week,ErrorOccurrence))
ncol(ModelData_Happen3) # 50

## Make the y-variable a factor
ModelData_Happen3$Happen_3_days <- as.factor(ModelData_Happen3$Happen_3_days)

```

```{r}
#set the seed
RNGkind(sample.kind = "default")
set.seed(2291352)

```

## Fit baseline random forest


 randomForest(formula = Happen_3_days ~ ., data = train.df_3,      ntree = 1000, mtry = sqrt(50), importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 1000
No. of variables tried at each split: 7

        OOB estimate of  error rate: 1.4%
Confusion matrix:
        no yes  class.error
no  125971  11 8.731406e-05
yes   1783 572 7.571125e-01
```{r}
train.idx_3 <- sample(x = 1:nrow(ModelData_Happen3), size = .7*nrow(ModelData_Happen3))
train.df_3 <- ModelData_Happen3[train.idx_3,]
test.df_3 <- ModelData_Happen3[-train.idx_3,]

baseline_forest_3 <- randomForest(Happen_3_days ~ . ,
                                data = train.df_3,
                                ntree = 1000,
                                mtry = sqrt(50), #sqrt(50) as number of x's to sample for each tree
                                importance = TRUE)

baseline_forest_3
```


## Model Tunning
```{r}
## 50 x variables
mtry <- c(1:50)

keeps_3 <- data.frame(m = rep(NA, length(mtry)),
                    OOB_error_rate = rep(NA, length(mtry)))

for (idx in 1:length(mtry)){
  
  print(paste0("Fitting m = ", mtry[idx]))
  
  tempforest_3 <- randomForest(Happen_3_days ~ . ,
                             data = train.df_3,
                             ntree = 1000,
                             mtry = mtry[idx]) #mtry is varying with idx
  
  
  #record the corresponding OOB error
  keeps_3[idx, "m"] <- mtry[idx]
  keeps_3[idx, "OOB_error_rate"] <-mean(predict(tempforest_3) != train.df_3$Happen_3_days)
  
}

keeps_3

```


## Tunning Results
```{r}
keeps_3


min_m_3 <- which.min(keeps_3$OOB_error_rate)  # find index of smallest value
min_error_rate_3 <- keeps_3$OOB_error_rate[min_m_3]  # get smallest error rate value
min_m_value_3 <- keeps_3$m[min_m_3]  # get corresponding m value

ggplot(data = keeps_3) +
  geom_line(aes(x = m, y = OOB_error_rate)) +
  geom_vline(xintercept = min_m_value_3, color = "red") + # add vertical line at minimum error rate
  scale_x_continuous(breaks = seq(0, 51, 5)) + # add more space between numbers on x axis
  labs(x = "m (mtry): # of x variables sampled",
       y = "OOB Error Rate") +
  annotate("text", x = min_m_value_3, y = min_error_rate_3, label = paste0("Min error rate at m = ", min_m_value_3), vjust = 1.5, color = "red") # add label at minimum error rate

```

## Fitting finalized model
Given that the least error happened when mtry = 6, I will use that tunned value on the following model
```{r}
TunnedForest_3 <- randomForest(Happen_3_days ~ .,
                            data = train.df_3,
                            ntree = 1000,
                            mtry = 6, #based on tuning above
                            importance = TRUE)

TunnedForest_3
```
Call:
 randomForest(formula = Happen_3_days ~ ., data = train.df_3,      ntree = 1000, mtry = 6, importance = TRUE) 
               Type of random forest: classification
                     Number of trees: 1000
No. of variables tried at each split: 6

        OOB estimate of  error rate: 1.43%
Confusion matrix:
        no yes  class.error
no  125972  10 7.937642e-05
yes   1827 528 7.757962e-01


## Saving the results of the tunned forest on a CSV
```{r}
#Let's save the results of that forest into a csv file
save(TunnedForest_3, file = "TurbineForest.RData")

```


## Storing the positive probabilities
pi_hat will store the probabilities of all positive events. The positive events mean that an Balance of Plant Error Type happened within 1 day and its represented as a "yes" in the Happen_1_day column
```{r}
pi_hat_3 <- predict(TunnedForest_3, test.df_3, type = 'prob')[,"yes"]
```


## Plot ROC Curve

```{r}
rocCurve_3 <- roc(response = test.df_3$Happen_3_days,
                predictor = pi_hat,
                levels = c("no","yes"))
plot(rocCurve_3, print.thres = TRUE, print.auc = TRUE)

```


```{r}
pi_star_3 <- coords(rocCurve_3, "best", ret = "threshold")$threshold[1]
pi_star_3 # 5e-04
test.df_3$forest_pred <- as.factor(ifelse(pi_hat_3 > pi_star_3, 'yes', 'no'))
View(test.df_3)
```

```{r}
y_hat_3 <- as.factor(ifelse(pi_hat_3 > pi_star_3, 'yes', 'no'))
table(y_hat_3, test.df_3$Happen_3_days)
```
y_hat_3    no   yes
    no  39315     0
    yes 14628  1059

```{r}
varImpPlot(TunnedForest_3, type = 1)
# Increase spacing between y-axis values
par(cex.axis = 3)  # adjust cex.axis to increase spacing
varImpPlot(TunnedForest_3, type = 1, horiz = TRUE, las = 1)
```




























































































































































































































































































